---
title: "INFS 692 - Final Project Model 3"
author: "Agata"
date: "2022-12-14"
output: pdf_document
---

### Libraries

```{r}

library(latexpdf)
library(rmarkdown)
library(readr)
library(plyr)
library(dplyr)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(COUNT)
library(caret)
library(rstatix)
library(modeldata)
library(rsample)    # for creating validation splits
library(recipes)    # for feature engineering
library(purrr)      #for mapping
library(tidyverse)  # for filtering 
library(ROCR)      # ROC Curves
library(pROC)      # ROC Curves
library(rpart)      # decision tree application
library(rpart.plot)  # plotting decision trees
library(vip)         # for feature importance
library(pdp) 


library(stringr)     # for string functionality

# Modeling packages
library(tidyverse)  # data manipulation
library(cluster)     # for general clustering algorithms
library(factoextra)  # for visualizing cluster results


# Modeling packages
library(mclust)   # for fitting clustering algorithms



```

### Importing Data

```{r}
data1 <- read.csv("radiomics_completedata.csv", sep = ",")
```

### Preprocessing data

Since we're not taking into consideration categorical variables or Failure.binary, I decided to split the data so that it only takes into consideration variables with Entropy. I thought this split had the most interesting results.


```{r}
#Check for null and missing values

which(is.null(data1))

which(is.na(data1))


#Data split


sub2 <- subset(data1, select= -c(Institution, Failure.binary, Failure))


sub2 <- dplyr::select(sub2, contains("Entropy"))




#Check for normality

hist(sub2$Entropy_cooc.W.ADC)

# or


sub1shapiro <- shapiro.test(sub2$Entropy_cooc.W.ADC)
sub1shapiro

#based on the histogram, the data is not normalized.This is also enhanced by the shapiro test
#where the p-value is < 0.05, which means that the data is not normalized.


#Normalize Data

scale_data <-  as.data.frame(scale(sub2, center = TRUE, scale = TRUE))

summary(scale_data$Entropy_cooc.W.ADC)
sd(scale_data$Entropy_cooc.W.ADC)

#Now the data has a mean of 0 and a standard deviation of 1, meaning the data is normalized. 


#check correlation for full data set except categorical variables

cor1 <- cor(scale_data)
#cor1 commented out for pdf page saving purposes

```
### K-Means

Based on the optimal number of clusters graph below, the optimal number is k=2, which is why the k-means clustering is set to 2.


```{r}
df <- scale_data


#Determining Optimal Number of Clusters
set.seed(123)

#function to compute total within-cluster sum of square 
wss <- function(k) {
  kmeans(df, k, nstart = 10 )$tot.withinss
}

# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15

# extract wss for 2-15 clusters
wss_values <- map_dbl(k.values, wss)

#or use this
fviz_nbclust(df, kmeans, method = "silhouette")


# Compute k-means clustering with k = 2
set.seed(123)
final <- kmeans(df, 2, nstart = 25)
print(final)

#final data
fviz_cluster(final, data = df)

```
### Hierarchical

```{r}
# For reproducibility
set.seed(123)

# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# For reproducibility
set.seed(123)

# Compute maximum or complete linkage clustering with agnes
hc2 <- agnes(df, method = "complete")

# Agglomerative coefficient
hc2$ac

# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)


# compute divisive hierarchical clustering
hc4 <- diana(df)

# Divise coefficient; amount of clustering structure found
hc4$dc



# plots to compare
p1 <- fviz_nbclust(df, FUN = hcut, method = "wss",
                   k.max = 10) +
  ggtitle("(A) Elbow method")
p2 <- fviz_nbclust(df, FUN = hcut, method = "silhouette",
                   k.max = 10) +
  ggtitle("(B) Silhouette method")
p3 <- fviz_nbclust(df, FUN = hcut, method = "gap_stat",
                   k.max = 10) +
  ggtitle("(A) Gap Statistics method")

gridExtra::grid.arrange(p1, p2, p3, nrow = 1)

# Ward's method

hc5 <- hclust(d, method = "ward.D2" )

dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])

```
As shown above, both silhouette and gap-statistics show the same optimal k number of clusters. Which is why we remain at k = 2.



```{r}
# Cut tree into 3 groups
sub_grp <- cutree(hc5, k = 2)

# Number of members in each cluster
table(sub_grp)

# Plot full dendogram
fviz_dend(
  hc5,
  k = 2,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco",
  cex = 0.1
)


dend_plot <- fviz_dend(hc5)                # create full dendogram
dend_data <- attr(dend_plot, "dendrogram") # extract plot info
dend_cuts <- cut(dend_data, h = 70.5)      # cut the dendogram at 
# designated height
# Create sub dendrogram plots
p1 <- fviz_dend(dend_cuts$lower[[1]])
p2 <- fviz_dend(dend_cuts$lower[[1]], type = 'circular')

# Side by side plots
gridExtra::grid.arrange(p1, p2, nrow = 1)

```
Based on this clustering algorithm, the final output of the dendrogram doesn't look as nice without zooming in. However, it does show more details than K-Means.


### Model-Based


```{r}
my_basket <- scale_data

# Apply GMM model with 3 components
data1_mc <- Mclust(scale_data, G = 3)

# Plot results

par(mar=c(1,1,1,1))


plot(data1_mc, what = "density")
plot(data1_mc, what = "uncertainty")

# Observations with high uncertainty
sort(data1_mc$uncertainty, decreasing = TRUE) %>% head()



data1_optimal_mc <- Mclust(scale_data)


legend_args <- list(x = "bottomright", ncol = 5)
plot(data1_optimal_mc, what = 'BIC', legendArgs = legend_args)
plot(data1_optimal_mc, what = 'classification')
plot(data1_optimal_mc, what = 'uncertainty')

my_basket_mc <- Mclust(my_basket, 1:20)


plot(my_basket_mc, what = 'BIC', 
     legendArgs = list(x = "bottomright", ncol = 5))

probabilities <- my_basket_mc$z 
colnames(probabilities) <- paste0('C', 1:3)

probabilities <- probabilities %>%
  as.data.frame() %>%
  mutate(id = row_number()) %>%
  tidyr::gather(cluster, probability, -id)

ggplot(probabilities, aes(probability)) +
  geom_histogram() +
  facet_wrap(~ cluster, nrow = 2)

uncertainty <- data.frame(
  id = 1:nrow(my_basket),
  cluster = my_basket_mc$classification,
  uncertainty = my_basket_mc$uncertainty
)

uncertainty %>%
  group_by(cluster) %>%
  filter(uncertainty > 0.25) %>%
  ggplot(aes(uncertainty, reorder(id, uncertainty))) +
  geom_point() +
  facet_wrap(~ cluster, scales = 'free_y', nrow = 1)


cluster2 <- my_basket %>%
  scale() %>%
  as.data.frame() %>%
  mutate(cluster = my_basket_mc$classification) %>%
  filter(cluster == 2) %>%
  select(-cluster)

cluster2 %>%
  tidyr::gather(product, std_count) %>%
  group_by(product) %>%
  summarize(avg = mean(std_count)) %>%
  ggplot(aes(avg, reorder(product, avg))) +
  geom_point() +
  labs(x = "Average Entropy", y = NULL)

```
### Conclusion

K-Means and Hierarchical models did not change their k cluster numbering, but model-based did with k = 3. Each model has an interesting way of displaying clusters, and the most interesting one of the three is the Hierarchical one with the dendrogram. 

